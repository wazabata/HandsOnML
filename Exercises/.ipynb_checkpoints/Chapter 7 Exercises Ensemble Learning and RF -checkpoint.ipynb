{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 -  Possible to have 5 different models that each have a 95% accuracy and build an ensemble model with these predictors that has an accuracy higher than 95% \n",
    "How: A predictor can be making more accurate predictions in instances where another predictor is making an inacurrate prediction\n",
    "\n",
    "What helps: \n",
    "- Very different models (independent models, that will make different types of errors) \n",
    "- Trained on different subsets of the data (bagging) \n",
    "- Use a voting ensemble, predicting the majority voted class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 - Difference between hard and soft voting classifier\n",
    "- The hard voting ensemble selects the majority voted class \n",
    "- The soft voting ensemble selects the class with the highest class probab, averaged over all individual classifiers (all classifiers need to be able to output class probab)\n",
    "---> High confidence classes are given more weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 - Bagging, pasting, RF models can be sped up by distributing across multiple servers, each predictor is built independently of the other \n",
    "\n",
    "On the other hand, boosting and stacking predictors are built on top of one another, and cannot be trained in // (predictors in the same layer can be trained in // w. stacking, but layers need to be trained sequentially) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 - Benefit of out of bag evaluation: Only around 63% of instances are sampled in bagging ensembles- 37% of training ensembles are not sampled and cannot therefore be used as out of bag samples. Instead of using less training data (w/ a train / val set split), can use the entire set knowing that ~1/3 of obs. won't be sampled in training set for each predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 - In random forests, each predictor (tree) uses a random subset of features to split each node\n",
    "With Extra Trees, the same applies, but instead of looking for the optimal split in this random subset of features, a random threshold is set for each feature. Therefore, Extra Trees can be thought of as regularized Random Forests and may perform better in the scenario where Random Forests overfit. \n",
    "\n",
    "Since ET don't search for optimal split, they can be trained faster than RFs (but predictions are the same)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6 - If AdaBoost underfits the training data, the hyperparameters that should be tweaked are: \n",
    "- Learning rate (increase) \n",
    "- n_estimators (increase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7 - To avoid overfitting in Gradient Boosting: \n",
    "- Decrease learning rate\n",
    "- Implement early stopping (too many estimators / predictors) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8 \n",
    "- Load MNIST data set\n",
    "- Train SVM, RF, Extra Trees, SVM \n",
    "- Bring them into ensemble (soft / hard voting) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-09d34dbe75aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfetch_openml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmnist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_openml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mnist_784'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfetch_mldata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from sklearn.datasets import fetch_openml\n",
    "    mnist = fetch_openml('mnist_784', version=1)\n",
    "    mnist.target = mnist.target.astype(np.int64)\n",
    "except ImportError:\n",
    "    from sklearn.datasets import fetch_mldata\n",
    "    mnist = fetch_mldata('MNIST original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mnist.data\n",
    "y = mnist.target\n",
    "\n",
    "## Get Test set \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size = 10000, random_state=42)\n",
    "\n",
    "## Get train and val set\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size = 10000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fit the models on the train set, no need for cross validation for now \n",
    "rf_clf = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "et_clf = ExtraTreesClassifier(n_estimators=10, random_state=42)\n",
    "svc_clf = LinearSVC(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the estimator:  RandomForestClassifier(n_estimators=10, random_state=42)\n",
      "Training the estimator:  ExtraTreesClassifier(n_estimators=10, random_state=42)\n",
      "Training the estimator:  LinearSVC(random_state=42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elieharik/Desktop/Dev/Books/Python/HandsOnML/env/lib/python3.7/site-packages/sklearn/svm/_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "## Train models \n",
    "estimators = [rf_clf, et_clf, svc_clf]\n",
    "\n",
    "for estimator in estimators: \n",
    "    print(\"Training the estimator: \", estimator)\n",
    "    estimator.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9469, 0.9492, 0.8695]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the score() method -> Return the mean accuracy on the given test data and labels.\n",
    "[estimator.score(X_val, y_val) for estimator in estimators]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elieharik/Desktop/Dev/Books/Python/HandsOnML/env/lib/python3.7/site-packages/sklearn/svm/_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('rf',\n",
       "                              RandomForestClassifier(n_estimators=10,\n",
       "                                                     random_state=42)),\n",
       "                             ('et',\n",
       "                              ExtraTreesClassifier(n_estimators=10,\n",
       "                                                   random_state=42)),\n",
       "                             ('svc', LinearSVC(random_state=42))])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Put predictors in training ensemble \n",
    "named_estimators = [\n",
    "    (\"rf\",rf_clf), \n",
    "    (\"et\",et_clf), \n",
    "    (\"svc\",svc_clf)\n",
    "]\n",
    "\n",
    "voting_classifier = VotingClassifier(named_estimators)\n",
    "voting_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9511"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_classifier.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9469, 0.9492, 0.8695]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[estimator.score(X_val, y_val) for estimator in voting_classifier.estimators_]\n",
    "\n",
    "## @note: \n",
    "## voting_classifier.estimators return an array of tuples - see *1 \n",
    "## On the other hand voting_classifier.estimators_ returns the models - see *2 \n",
    "\n",
    "\n",
    "## *1\n",
    "# [('rf', RandomForestClassifier(n_estimators=10, random_state=42)),\n",
    "#  ('et', ExtraTreesClassifier(n_estimators=10, random_state=42)),\n",
    "#  ('svc', LinearSVC(random_state=42))]\n",
    "\n",
    "## *2 \n",
    "# [RandomForestClassifier(n_estimators=10, random_state=42),\n",
    "#  ExtraTreesClassifier(n_estimators=10, random_state=42),\n",
    "#  LinearSVC(random_state=42)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[RandomForestClassifier(n_estimators=10, random_state=42),\n",
       " ExtraTreesClassifier(n_estimators=10, random_state=42)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Remove SVC - underperforms \n",
    "del voting_classifier.estimators_[2]\n",
    "voting_classifier.estimators_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9445"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_classifier.score(X_val, y_val) ## Bizarrily enough, underperforms! Probably RF and ET performing same errors, models are too similar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9595"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_classifier.voting = 'soft'\n",
    "voting_classifier.score(X_val, y_val) ## Better "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting classifier score:  0.961\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9437, 0.9474]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Voting classifier score: \", voting_classifier.score(X_test, y_test))\n",
    "\n",
    "[estimator.score(X_test, y_test) for estimator in voting_classifier.estimators_]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run classifiers aboive to make predicftions on validation set \n",
    "## Create new training set with predictions \n",
    "##    -> Each training instance is a vector containing the set of predictions from all your classifiers for an image, target is image's class \n",
    "\n",
    "## Train a classifier on new training set --> Have a blender and form a stacking ensemble \n",
    "\n",
    "## Evaluate ensemble on teset set \n",
    "##   For each image in test set, make prediction with classifiers, then feed predictions to the blender to get ensemble predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training estimator:  RandomForestClassifier(n_estimators=10, random_state=42)\n",
      "Training estimator:  ExtraTreesClassifier(n_estimators=10, random_state=42)\n",
      "Training estimator:  LinearSVC(random_state=42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elieharik/Desktop/Dev/Books/Python/HandsOnML/env/lib/python3.7/site-packages/sklearn/svm/_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "## retrain predictors in case\n",
    "estimators = [rf_clf, et_clf, svc_clf]\n",
    "\n",
    "\n",
    "for estimator in estimators: \n",
    "    print(\"Training estimator: \", estimator)\n",
    "    estimator.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make preditions on validation set \n",
    "val_predictions = np.empty((len(X_val), len(estimators)), dtype=np.float32)\n",
    "\n",
    "for index, estimator in enumerate(estimators): \n",
    "    val_predictions[:, index] = estimator.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=200, oob_score=True, random_state=42)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_forest_blender = RandomForestClassifier(n_estimators=200, oob_score=True, random_state=42)\n",
    "rnd_forest_blender.fit(val_predictions, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.951"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_forest_blender.oob_score_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['8', '4', '8', ..., '3', '8', '3'], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_predictions = np.empty((len(X_test), len(estimators)), dtype=np.float32)\n",
    "\n",
    "for index, estimator in enumerate(estimators): \n",
    "    X_test_predictions[:, index] = estimator.predict(X_test)\n",
    "\n",
    "y_pred = rnd_forest_blender.predict(X_test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
